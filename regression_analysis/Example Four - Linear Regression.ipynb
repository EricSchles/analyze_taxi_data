{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "Thus far we've seen descriptive statistics, filtering, conformitory analysis - all of that comes together in modeling.\n",
    "\n",
    "In a way we've already been doing statistical modeling, because statistical modeling is merely the sum of the parts we've covered thus far.\n",
    "\n",
    "Generally speaking statistics, mathematical measures which describe data, can be broken out into a few categories:\n",
    "\n",
    "* L-estimators\n",
    "* M-estimators\n",
    "* \"Advanced\" estimators\n",
    "\n",
    "So far we've been looking at L-estimators, these are things like the mean, the median, the standard deviation or the interquartile range.\n",
    "\n",
    "Each of these measures has a clear and _simple_ mathematical description with a clear and simple intuition for humans.  As an aside, because these estimators are so simple, people can often misinterpret their results, by misunderstanding the underlying data or not ensuring all assumptions of the L-estimator are satisfied.  This is so called, \"lying with statistics\".\n",
    "\n",
    "There is nothing new with M-estimators like the one we'll look at here, or the ones we'll look at in the next exercise, except for complexity.  \n",
    "\n",
    "Our first example of an M-estimator is called Ordinary Least Squares.  It is given it's name because of how the estimator works and how it's used.\n",
    "\n",
    "With the L-estimators we looked at previously we need only look at a single column.  This is in part because we were learning simple patterns - things like the center or spread of a single column.  \n",
    "\n",
    "Now, with M-estimators we'll be looking at things like the strength of the relationship between one or more related variables.  Also the description our M-estimator will produce won't be a simple number, instead it will be equation, which is an approximation of the relationships of the underlying data.\n",
    "\n",
    "If this equation is a reasonable approximation, then a whole set of truths fall out from this equation, and we can leverage all of the relevant pieces of mathematics to inform our analysis.\n",
    "\n",
    "For instance, with an equation like:\n",
    "\n",
    "`Fair_amount = 2*Trip_distance + epsilon`\n",
    "\n",
    "Here epsilon is some small amount of noise which is normally distributed with mean 0 and standard deviation 1.\n",
    "\n",
    "If the above equation holds true we can make informed decisions about when to take a cab!  But more than that - we know the derivative of this equation, the visual graph of this function and many other facts about this relationship between `Fair_amount` and `Trip_distance`.  Of course, 2 is just a made up coefficient that probably isn't accurate.\n",
    "\n",
    "Using Ordinary Least Squares, we'll be able to figure out what the real coefficient is.  And we can use that information to inform an analysis of taxi cabs across New York City.\n",
    "\n",
    "## A First Example\n",
    "\n",
    "Now that we've got a basic understand of the goals of linear regression, let's see how it works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3d54a739998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandint\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp_randint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Now that we have multiple variables some sense of summary will be helpful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel, f_regression\n",
    "import math\n",
    "\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn import metrics\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Now that we have multiple variables some sense of summary will be helpful\n",
    "def summary(X_vars, y_var, model, categorical=False):\n",
    "    cols = X_vars.columns.tolist()\n",
    "    # checks to see if the variable is categorical\n",
    "    if categorical == True:\n",
    "        lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_vars, y_var)\n",
    "        model = SelectFromModel(lsvc, prefit=True)\n",
    "        labels = [cols[x] for x in model.get_support(indices=True) if x]\n",
    "        X_new = model.transform(X_vars)\n",
    "        print(\"Linear SVC Feature reduction\")\n",
    "        print(cols, \"->\", labels)\n",
    "    f_test, t_test_pvals = f_regression(X_vars, y_var)\n",
    "    t_test = [math.sqrt(elem) for elem in f_test]\n",
    "    f_statistic, f_prob = stats.f_oneway(*[np.array(X_vars[column]) for column in X_vars.columns])\n",
    "    return f_statistic, f_prob, t_test, t_test_pvals, model.coef_\n",
    "    \n",
    "        \n",
    "def pretty_print_results(X, f_statistic, f_prob, t_test, t_test_pvals, mi, coef):\n",
    "    cols = X.columns.tolist()\n",
    "    t_test = list(t_test)\n",
    "    t_test_pvals = list(t_test_pvals)\n",
    "    mi = list(mi)\n",
    "    coef = list(coef)\n",
    "    tables = []\n",
    "    if len(X) > 3:\n",
    "        for index in range(0, len(X), 3):\n",
    "            if abs(len(X) - index) < 3:\n",
    "                tmp_col = cols[index:] \n",
    "                tmp_t_test = t_test[index:]\n",
    "                tmp_t_test_pvals = t_test_pvals[index:]\n",
    "                tmp_mi = mi[index:]\n",
    "                tmp_coef = coef[index:]\n",
    "            else:    \n",
    "                tmp_col = cols[index:index+3] \n",
    "                tmp_t_test = t_test[index:index+3]\n",
    "                tmp_t_test_pvals = t_test_pvals[index:index+3]\n",
    "                tmp_mi = mi[index:index+3]\n",
    "                tmp_coef = coef[index:index+3]\n",
    "            if tmp_col == []:\n",
    "                break\n",
    "            tmp_table = PrettyTable([\"test_name\"] + tmp_col)    \n",
    "            tmp_table.add_row([\"t-test\"] + tmp_t_test)\n",
    "            tmp_table.add_row([\"t-test pvals\"] + tmp_t_test_pvals)\n",
    "            tmp_table.add_row([\"mutual info\"] + tmp_mi)\n",
    "            tmp_table.add_row([\"coefficient\"] + tmp_coef)\n",
    "            tables.append(tmp_table)\n",
    "            \n",
    "    else:\n",
    "        tmp_table = PrettyTable([\"test_name\"] + col)    \n",
    "        tmp_table.add_row([\"t-test\"] + t_test)\n",
    "        tmp_table.add_row([\"t-test pvals\"] + t_test_pvals)\n",
    "        tmp_table.add_row([\"mutual info\"] + mi)\n",
    "        tmp_table.add_row([\"coefficient\"] + coef)\n",
    "        tables.append(tmp_table)\n",
    "    print(\"F-statistic\", f_statistic)\n",
    "    print(\"F-prob\", f_prob)\n",
    "    for table in tables:\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy\n",
    "import numpy as np\n",
    "\n",
    "def generate_data():\n",
    "    X = np.random.randn(1000, 10)\n",
    "    y = X*2\n",
    "    return X, y\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "X, y = generate_data()\n",
    "\n",
    "linear_regression.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51201876,  0.47426296, -0.10556103, -1.17890058,  0.77113891,\n",
       "       -0.42845718,  0.37776175,  0.49112643, -0.41052297, -0.11299925])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1000, 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.statistical_based import chi_square\n",
    "from skfeature.function.information_theoretical_based import CIFE\n",
    "from skfeature.function.statistical_based import CFS\n",
    "from skfeature.function.information_theoretical_based import CMIM\n",
    "from skfeature.function.information_theoretical_based import DISR\n",
    "from skfeature.function.information_theoretical_based import FCBF\n",
    "from skfeature.function.information_theoretical_based import ICAP\n",
    "from skfeature.function.information_theoretical_based import JMI\n",
    "from skfeature.function.information_theoretical_based import MIFS\n",
    "from skfeature.function.information_theoretical_based import MIM\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "from skfeature.function.similarity_based import SPEC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
