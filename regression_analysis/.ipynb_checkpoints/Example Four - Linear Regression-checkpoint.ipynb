{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "Thus far we've seen descriptive statistics, filtering, conformitory analysis - all of that comes together in modeling.\n",
    "\n",
    "In a way we've already been doing statistical modeling, because statistical modeling is merely the sum of the parts we've covered thus far.\n",
    "\n",
    "Generally speaking statistics, mathematical measures which describe data, can be broken out into a few categories:\n",
    "\n",
    "* L-estimators\n",
    "* M-estimators\n",
    "* \"Advanced\" estimators\n",
    "\n",
    "So far we've been looking at L-estimators, these are things like the mean, the median, the standard deviation or the interquartile range.\n",
    "\n",
    "Each of these measures has a clear and _simple_ mathematical description with a clear and simple intuition for humans.  As an aside, because these estimators are so simple, people can often misinterpret their results, by misunderstanding the underlying data or not ensuring all assumptions of the L-estimator are satisfied.  This is so called, \"lying with statistics\".\n",
    "\n",
    "There is nothing new with M-estimators like the one we'll look at here, or the ones we'll look at in the next exercise, except for complexity.  \n",
    "\n",
    "Our first example of an M-estimator is called Ordinary Least Squares.  It is given it's name because of how the estimator works and how it's used.\n",
    "\n",
    "With the L-estimators we looked at previously we need only look at a single column.  This is in part because we were learning simple patterns - things like the center or spread of a single column.  \n",
    "\n",
    "Now, with M-estimators we'll be looking at things like the strength of the relationship between one or more related variables.  Also the description our M-estimator will produce won't be a simple number, instead it will be equation, which is an approximation of the relationships of the underlying data.\n",
    "\n",
    "If this equation is a reasonable approximation, then a whole set of truths fall out from this equation, and we can leverage all of the relevant pieces of mathematics to inform our analysis.\n",
    "\n",
    "For instance, with an equation like:\n",
    "\n",
    "`Fair_amount = 2*Trip_distance + epsilon`\n",
    "\n",
    "Here epsilon is some small amount of noise which is normally distributed with mean 0 and standard deviation 1.\n",
    "\n",
    "If the above equation holds true we can make informed decisions about when to take a cab!  But more than that - we know the derivative of this equation, the visual graph of this function and many other facts about this relationship between `Fair_amount` and `Trip_distance`.  Of course, 2 is just a made up coefficient that probably isn't accurate.\n",
    "\n",
    "Using Ordinary Least Squares, we'll be able to figure out what the real coefficient is.  And we can use that information to inform an analysis of taxi cabs across New York City.\n",
    "\n",
    "## A First Example\n",
    "\n",
    "Now that we've got a basic understand of the goals of linear regression, let's see how it works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.651e+32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 24 Jul 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:25:15</td>     <th>  Log-Likelihood:    </th> <td>  3273.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -6540.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>  -6532.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-3.331e-16</td> <td>  1.5e-16</td> <td>   -2.215</td> <td> 0.029</td> <td>-6.31e-16</td> <td>-3.46e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    2.0000</td> <td>  1.6e-16</td> <td> 1.25e+16</td> <td> 0.000</td> <td>    2.000</td> <td>    2.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    3.0000</td> <td> 1.53e-16</td> <td> 1.96e+16</td> <td> 0.000</td> <td>    3.000</td> <td>    3.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.034</td> <th>  Durbin-Watson:     </th> <td>   2.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.362</td> <th>  Jarque-Bera (JB):  </th> <td>   1.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.331</td> <th>  Prob(JB):          </th> <td>   0.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.954</td> <th>  Cond. No.          </th> <td>    1.11</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       1.000\n",
       "Model:                            OLS   Adj. R-squared:                  1.000\n",
       "Method:                 Least Squares   F-statistic:                 2.651e+32\n",
       "Date:                Tue, 24 Jul 2018   Prob (F-statistic):               0.00\n",
       "Time:                        17:25:15   Log-Likelihood:                 3273.0\n",
       "No. Observations:                 100   AIC:                            -6540.\n",
       "Df Residuals:                      97   BIC:                            -6532.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const      -3.331e-16    1.5e-16     -2.215      0.029   -6.31e-16   -3.46e-17\n",
       "x1             2.0000    1.6e-16   1.25e+16      0.000       2.000       2.000\n",
       "x2             3.0000   1.53e-16   1.96e+16      0.000       3.000       3.000\n",
       "==============================================================================\n",
       "Omnibus:                        2.034   Durbin-Watson:                   2.038\n",
       "Prob(Omnibus):                  0.362   Jarque-Bera (JB):                1.838\n",
       "Skew:                          -0.331   Prob(JB):                        0.399\n",
       "Kurtosis:                       2.954   Cond. No.                         1.11\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def generate_data():\n",
    "    X = np.zeros((100, 2))\n",
    "    for index in range(100):\n",
    "        X[index] = np.random.normal(0, 1, 2)\n",
    "    y = X[:,0]*2 + X[:,1]*3\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data()\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y,X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting our Model\n",
    "\n",
    "Now that we've seen our model, let's interpret it:\n",
    "\n",
    "## High level point\n",
    "\n",
    "For linear models the only thing we are measuring is the interplay in variance between the independent and dependent variables.\n",
    "\n",
    "## Goodness of fit\n",
    "\n",
    "The first thing we want to look at is the R^2 score.  The R^2 is a measure of \"goodness of fit\" of our model.  If our model fits our data well, then we have a reasonable level of understand of how our independent variable (y) is effected by our dependent variables (X).  \n",
    "\n",
    "It's worth noting that R^2 is only an effective measure of fit for linear models.  It is not good for capturing non-linear fit, because of an underlying assumption of R^2.  R^2 captures goodness of fit by looking first assuming linear regression was performed and then looks at the sum of squares variance for the independent and dependent variables.\n",
    "\n",
    "For interpretability purposes, we can think of R^2 as the variance in X that explains variance in Y.  \n",
    "\n",
    "So for the above example, 100% of the variance in X explains 100% of the variance in y.  And thereby y is fully explained by X, in a statistical correlation sense.  \n",
    "\n",
    "\n",
    "## Are any of the variables important?\n",
    "\n",
    "The next measure usally under consideration is the F-Test which asks, are all the variables taken together statistically significant for the dependent variable (y)?\n",
    "\n",
    "If the p-value is above 0.05 we fail to reject the null hypothesis, meaning the variables are probably not good predictors of y.  Which means we should choose new variables for predictors.  If the p-value is below 0.05 then we can likely safely say some of the independent variables (X) are statistically related to the independent variable (y).\n",
    "\n",
    "Here we get a p-value of 0.00 therefore we can conclude all the values are jointly statistically significant.\n",
    "\n",
    "## Are each of the variables important?\n",
    "\n",
    "The last question we ask is with respect to individual variable importance.  This is done via the t-test.  This asks the question, is the given variable statistically significant with respect to the independent variable?  \n",
    "\n",
    "So we'll have one t-test per variable -\n",
    "\n",
    "As you can see above X1 and X2 are both statistically significant with respect to y because both of their pvalues are 0.00\n",
    "\n",
    "## Some important caveats about t - tests\n",
    "\n",
    "t-tests are defined by\n",
    "\n",
    "`coefficient of variable / variance of variable`\n",
    "\n",
    "So this means that a t - test can pass for one of a few reasons:\n",
    "\n",
    "1. the coefficient is very very small\n",
    "2. the variance is very high\n",
    "3. Given a reasonable coefficient magnitude, the interplay between the size of the coefficient and the variance of the variable is below 0.05\n",
    "\n",
    "If we are trying to ensure our tests pass for the right reasons, we want to be in case three.  If we are in case 1 - specifically the coefficient is very small, the t test might pass incorrectly, because the variance of the independent variable may not actually contribute to variance of the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Now that we have a sense of how linear regression works, let's make a new dataset with some features that matter and some features that don't.  Then we'll show how our model tests inform how we decide which independent variables matter for statistical significance.\n",
    "\n",
    "Then we'll compare this with automated feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.statistical_based import chi_square\n",
    "from skfeature.function.information_theoretical_based import CIFE\n",
    "from skfeature.function.statistical_based import CFS\n",
    "from skfeature.function.information_theoretical_based import CMIM\n",
    "from skfeature.function.information_theoretical_based import DISR\n",
    "from skfeature.function.information_theoretical_based import FCBF\n",
    "from skfeature.function.information_theoretical_based import ICAP\n",
    "from skfeature.function.information_theoretical_based import JMI\n",
    "from skfeature.function.information_theoretical_based import MIFS\n",
    "from skfeature.function.information_theoretical_based import MIM\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "from skfeature.function.similarity_based import SPEC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
