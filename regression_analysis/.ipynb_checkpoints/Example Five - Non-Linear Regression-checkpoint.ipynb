{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample = pd.read_csv(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.45555204325186405\n",
      "Fitting with GridSearch\n",
      "Model Rank 1\n",
      "Mean validation Score -0.7824587185275295\n",
      "Std of validation score 0.15722681556783957\n",
      "Parameters: {'criterion': 'friedman_mse', 'max_depth': 20, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Model Rank 2\n",
      "Mean validation Score -0.7942518748648246\n",
      "Std of validation score 0.11336443601311968\n",
      "Parameters: {'criterion': 'mse', 'max_depth': 20, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Model Rank 3\n",
      "Mean validation Score -0.7975921645310559\n",
      "Std of validation score 0.2292870913008529\n",
      "Parameters: {'criterion': 'mse', 'max_depth': None, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "feature importance\n",
      "Tip_amount 0.937063908616152\n",
      "Total_amount 0.042881903909262734\n",
      "Tolls_amount 2.1869129472645814e-05\n",
      "improvement_surcharge 0.0007382614635051287\n",
      "Trip_distance 0.01929405688160748\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_trip_distance_price_feature_dec_tree(sample):\n",
    "    X = sample[[\"Tip_amount\", \n",
    "                \"Total_amount\",\n",
    "                \"Tolls_amount\",\n",
    "                \"improvement_surcharge\",\n",
    "                \"Trip_distance\"]]\n",
    "    y = sample[\"tip_percentage\"]\n",
    "    dec_tree = tree.DecisionTreeRegressor()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33)\n",
    "    dec_tree.fit(X_train, y_train)\n",
    "    y_pred = dec_tree.predict(X_test)\n",
    "    print(\"MSE\", metrics.mean_squared_error(y_test, y_pred))\n",
    "    num_features = len(X)\n",
    "    print(\"Fitting with GridSearch\")\n",
    "    param_grid = {\"criterion\": [\"mse\", \"friedman_mse\"],\n",
    "              \"min_samples_split\": [2, 10, 20],\n",
    "              \"max_depth\": [None, 2, 10, 20],\n",
    "              \"min_samples_leaf\": [1, 5, 10],\n",
    "              \"max_leaf_nodes\": [None, 5, 10, 20],\n",
    "              }\n",
    "    \n",
    "    dec_tree = tree.DecisionTreeRegressor()\n",
    "    grid_search = GridSearchCV(dec_tree,\n",
    "                              param_grid=param_grid,\n",
    "                              cv=3, scoring=\"neg_mean_squared_error\")\n",
    "    grid_search.fit(X, y)\n",
    "    grid_scores = grid_search.grid_scores_\n",
    "    top_scores = sorted(grid_scores,\n",
    "                       key=lambda t:t[1],\n",
    "                        reverse=True)[:3]\n",
    "    for index, score in enumerate(top_scores):\n",
    "        print(\"Model Rank\", index+1)\n",
    "        print(\"Mean validation Score\", score.mean_validation_score)\n",
    "        print(\"Std of validation score\", np.std(score.cv_validation_scores))\n",
    "        print(\"Parameters:\", score.parameters)\n",
    "    print(\"feature importance\")\n",
    "    feature_importance = grid_search.best_estimator_.feature_importances_\n",
    "    for index, col in enumerate(X.columns):\n",
    "        print(col, feature_importance[index])\n",
    "        \n",
    "test_trip_distance_price_feature_dec_tree(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.92      0.90      9780\n",
      "          1       0.88      0.82      0.85      6720\n",
      "\n",
      "avg / total       0.88      0.88      0.88     16500\n",
      "\n",
      "feature importance:\n",
      "Total_amount 0.4223900787243231\n",
      "Tolls_amount 0.005964709396045839\n",
      "improvement_surcharge 0.007470562005818309\n",
      "Trip_distance 0.5641746498738129\n"
     ]
    }
   ],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "from sklearn import ensemble\n",
    "\n",
    "def test_time_gbc(sample):\n",
    "    map_values = {\n",
    "        False: 0,\n",
    "        True: 1\n",
    "    }\n",
    "    y = sample[\"tip_percentage\"] != 0\n",
    "    y = y.map(map_values)\n",
    "    X = sample[[\"Total_amount\",\n",
    "                \"Tolls_amount\",\n",
    "                \"improvement_surcharge\",\n",
    "                \"Trip_distance\"]]\n",
    "    num_features = len(X)\n",
    "    params = {\"criterion\": \"friedman_mse\",\n",
    "              \"min_samples_split\": 10, \n",
    "              \"max_depth\": num_features*3,\n",
    "              \"min_samples_leaf\": 5,\n",
    "              \"max_leaf_nodes\": None,\n",
    "              \"warm_start\": True\n",
    "              }\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33)\n",
    "    gbc = ensemble.GradientBoostingClassifier(**params)\n",
    "    \n",
    "    gbc.fit(X_train, y_train)\n",
    "    y_pred_test = gbc.predict(X_test)\n",
    "    print()\n",
    "    print(\"F1:\", metrics.classification_report(y_test, y_pred_test))\n",
    "    print(\"feature importance:\")\n",
    "    feature_importances = gbc.feature_importances_\n",
    "    for index, column in enumerate(X.columns):\n",
    "        print(column, feature_importances[index])\n",
    "    y_pred_train = gbc.predict(X_train)    \n",
    "    tree_clf = tree.DecisionTreeClassifier()\n",
    "    tree_model = tree_clf.fit(X_train, y_pred_train)\n",
    "\n",
    "    dot_data = tree.export_graphviz(tree_model, out_file=None, \n",
    "                             feature_names=X_train.columns,  \n",
    "                             class_names=\"tip_percentage\",  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True)  \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "    picture = Image(graph.create_png())\n",
    "    return picture\n",
    "    \n",
    "picture = test_time_gbc(sample)\n",
    "picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fe243ebd803f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_distance_time_price_feature_xgbr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     X = sample[[\"Tip_amount\", \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def test_distance_time_price_feature_xgbr(sample):\n",
    "    X = sample[[\"Tip_amount\", \n",
    "                \"Total_amount\",\n",
    "                \"Tolls_amount\",\n",
    "                \"improvement_surcharge\",\n",
    "                \"Trip_distance\"]]\n",
    "    y = sample[\"tip_percentage\"]\n",
    "    \n",
    "    xgb1 = XGBRegressor()\n",
    "    parameters = {'objective':['reg:linear'],\n",
    "                  'learning_rate': [.03, 0.05], \n",
    "                  'max_depth': [5, 10, 15],\n",
    "                  'min_child_weight': [5, 10, 15],\n",
    "                  'silent': [1],\n",
    "                  'subsample': [0.3, 0.5],\n",
    "                  'colsample_bytree': [0.3, 0.5],\n",
    "                  'colsample_bylevel': [0.3, 0.5],\n",
    "                  'reg_lambda':[0.5, 1, 5],\n",
    "                  'n_estimators': [50, 100, 500]}\n",
    "\n",
    "    grid_search = GridSearchCV(xgb1,\n",
    "                            parameters,\n",
    "                            cv = 2,\n",
    "                            n_jobs = 5,\n",
    "                            verbose=True,\n",
    "                            scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "    grid_scores = grid_search.grid_scores_\n",
    "    top_scores = sorted(grid_scores,\n",
    "                       key=lambda t:t[1],\n",
    "                        reverse=True)[:3]\n",
    "    for index, score in enumerate(top_scores):\n",
    "        print(\"Model Rank\", index+1)\n",
    "        print(\"Mean validation Score\", score.mean_validation_score)\n",
    "        print(\"Std of validation score\", np.std(score.cv_validation_scores))\n",
    "        print(\"Parameters:\", score.parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Feature Engineering Strategies (Optional)\n",
    "\n",
    "As we saw from the last section feature engineering requires a decent understanding of the data, because you'll need to understand what size are reasonable for the coefficients, but also you'll need to understand why t-tests pass or fail.  If you have that understanding of the data and the magnitudes of the various coefficients then you will always be able to build a stronger statistical model by making use of manual feature engineering.\n",
    "\n",
    "That said manual feature engineering isn't always the best strategy for feature engineering.  It very much depends on the data.  If the data has features that aren't intuitive to a human then it's likely an automated feature engineering strategy will be superior.\n",
    "\n",
    "Examples of types of data when automated strategies will do better include:\n",
    "\n",
    "* dealing with text processing\n",
    "* dealing with image processing\n",
    "* dealing with video processing\n",
    "\n",
    "The reason automated feature engineering tends to do better here should be obvious - the features generated from the data don't make sense to a human the same way that the data we've been dealing with thus far does.\n",
    "\n",
    "## Specific Strategies\n",
    "\n",
    "* Feature Ranking\n",
    "* Recursive Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.statistical_based import chi_square\n",
    "from skfeature.function.information_theoretical_based import CIFE\n",
    "from skfeature.function.statistical_based import CFS\n",
    "from skfeature.function.information_theoretical_based import CMIM\n",
    "from skfeature.function.information_theoretical_based import DISR\n",
    "from skfeature.function.information_theoretical_based import FCBF\n",
    "from skfeature.function.information_theoretical_based import ICAP\n",
    "from skfeature.function.information_theoretical_based import JMI\n",
    "from skfeature.function.information_theoretical_based import MIFS\n",
    "from skfeature.function.information_theoretical_based import MIM\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "from skfeature.function.similarity_based import SPEC\n",
    "\n",
    "def feature_selection_automation(sample):\n",
    "    map_values = {\n",
    "        False: 0,\n",
    "        True: 1\n",
    "    }\n",
    "    y = sample[\"tip_percentage\"] != 0\n",
    "    y = y.map(map_values)\n",
    "    columns = sample.columns.tolist()\n",
    "    columns.remove(\"tip_percentage\")\n",
    "    X = sample[columns]\n",
    "    \n",
    "    features_per_algorithm = {}\n",
    "    score = chi_square.chi_square(X, y)\n",
    "    chi_idx = chi_square.feature_ranking(score)\n",
    "    features_per_algorithm[\"chi\"] = [columns[index] \n",
    "                                     for index in chi_idx[0:8]]\n",
    "    cfs_idx = CFS.cfs(X.as_matrix(), y.as_matrix())\n",
    "    features_per_algorithm[\"cfs\"] = [columns[index] \n",
    "                                     for index in cfs_idx[0:8]]\n",
    "    cife_idx = CIFE.cife(X.as_matrix(), y.as_matrix())\n",
    "    features_per_algorithm[\"cife\"] = [columns[index] \n",
    "                                     for index in cife_idx[0:8]]\n",
    "    cmim_idx = CMIM.cmim(X.as_matrix(), y.as_matrix())\n",
    "    features_per_algorithm[\"cmim\"] = [columns[index] \n",
    "                                     for index in cmim_idx[0:8]]\n",
    "    disr_idx = DISR.disr(X.as_matrix(), y.as_matrix(),\n",
    "                         n_selected_features=8)\n",
    "    features_per_algorithm[\"disr\"] = [columns[index] \n",
    "                                     for index in disr_idx[0:8]]\n",
    "    fcbf_idx = FCBF.fcbf(X.as_matrix(), y.as_matrix(),\n",
    "                         n_selected_features=8)\n",
    "    features_per_algorithm[\"fcbf\"] = [columns[index] \n",
    "                                     for index in fcbf_idx[0:8]]\n",
    "    icap_idx = ICAP.icap(X.as_matrix(), y.as_matrix(),\n",
    "                         n_selected_features=8)\n",
    "    features_per_algorithm[\"icap\"] = [columns[index] \n",
    "                                     for index in icap_idx[0:8]]\n",
    "    jmi_idx = JMI.jmi(X.as_matrix(), y.as_matrix(),\n",
    "                         n_selected_features=8)\n",
    "    features_per_algorithm[\"jmi\"] = [columns[index] \n",
    "                                     for index in jmi_idx[0:8]]\n",
    "    mifs_idx = MIFS.mifs(X.as_matrix(), y.as_matrix(),\n",
    "                         n_selected_features=8)\n",
    "    features_per_algorithm[\"mifs\"] = [columns[index] \n",
    "                                     for index in mifs_idx[0:8]]\n",
    "    mim_idx = MIM.mim(X.as_matrix(), y.as_matrix(),\n",
    "                         n_selected_features=8)\n",
    "    features_per_algorithm[\"mim\"] = [columns[index] \n",
    "                                     for index in mim_idx[0:8]]\n",
    "    mrmr_idx = MRMR.mrmr(X.as_matrix(), y.as_matrix(),\n",
    "                         n_selected_features=8)\n",
    "    features_per_algorithm[\"mrmr\"] = [columns[index] \n",
    "                                     for index in mrmr_idx[0:8]]   \n",
    "    features_across_all_algorithms = []\n",
    "    for column in columns:\n",
    "        tmp = []\n",
    "        for algo in features_per_algorithm:\n",
    "            tmp.append(column in features_per_algorithm[algo])\n",
    "        if all(tmp):\n",
    "            features_across_all_algorithms.append(column)\n",
    "    return features_per_algorithm, features_across_all_algorithms\n",
    "\n",
    "cols = sample.columns.tolist()\n",
    "cols.remove(\"VendorID\")\n",
    "cols.remove(\"lpep_pickup_datetime\")\n",
    "cols.remove(\"Lpep_dropoff_datetime\")\n",
    "cols.remove(\"Store_and_fwd_flag\")\n",
    "cols.remove(\"RateCodeID\")\n",
    "cols.remove(\"Pickup_longitude\")\n",
    "cols.remove(\"Pickup_latitude\")\n",
    "cols.remove(\"Dropoff_longitude\")\n",
    "cols.remove(\"Dropoff_latitude\")\n",
    "subsample = sample[cols]\n",
    "subsample = subsample.replace([np.inf, -np.inf], np.nan)\n",
    "subsample = subsample[pd.notnull(subsample[\"ave_speed\"])]\n",
    "features_per_algorithm, features_across_all = feature_selection_automation(subsample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
